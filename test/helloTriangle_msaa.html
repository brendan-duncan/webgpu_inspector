<html>
    <body>
        <canvas id="webgpu_canvas" width="960" height="600"></canvas>
        <script>
async function main() {
  // Check for basic WebGPU support.
  if (!navigator.gpu) {
    console.warning("WebGPU is not supported on this browser.");
    return;
  }

  // Get the canvas from the page
  const canvas = document.getElementById("webgpu_canvas");
  
  // Request the Adapter from the browser GPU object.
  // The Adapter will be null
  const adapter = await navigator.gpu.requestAdapter();
  // If the adapter is null, that means WebGPU is not
  // available on this particular device. This can happen
  // on some platforms like Android.
  if (!adapter) {
    console.warning("WebGPU is not supported on this browser.");
    return;
  }
  // Request the device from the Adapter. The Device is
  // the access point to WebGPU graphics commands.
  const device = await adapter.requestDevice();

  // The GPU object will tell you what the texture format of the canvas
  // render texture should be.
  const presentationFormat = navigator.gpu.getPreferredCanvasFormat();

  // Get the WebGPU context from
  const context = canvas.getContext("webgpu");

  // Configure the canvas WebGPU context so it can be rendered to by
  // this device. We also say what the render texture pixel format
  // is...which is what WebGPU told us it should be.
  context.configure({ device: device, format: presentationFormat });

  // WebGPU Shading Language (WGSL) can have both the fragment and
  // vertex shaders in the same shader code, or they could be separate
  // GPUShaderModule"s.
  const redTriangleShader = device.createShaderModule({
    code: `
      @vertex
      fn vertexMain(@builtin(vertex_index) VertexIndex : u32) -> @builtin(position) vec4<f32> {
        var pos = array<vec2<f32>, 3>(
          vec2(0.0, 0.5),
          vec2(-0.5, -0.5),
          vec2(0.5, -0.5));
        return vec4<f32>(pos[VertexIndex], 0.0, 1.0);
      }

      struct Out {
        @location(0) color1 : vec4<f32>,
        @location(1) color2 : vec4<f32>
      };

      @fragment
      fn fragmentMain() -> Out {
        return Out(vec4<f32>(1.0, 0.0, 0.0, 1.0), vec4<f32>(0.0, 0.0, 1.0, 1.0));
      }`,
  });

  // Create a RenderPipeline. This includes all of the state needed
  // to draw geometry, including what shaders to use, the topology, etc.
  // You have to include a PipelineLayout, which describes the resources
  // the shader will use, but for simple cases you can set it to "auto"
  // and let WebGPU figure it out.
  const pipeline = device.createRenderPipeline({
    layout: "auto",
    vertex: {
      module: redTriangleShader,
      entryPoint: "vertexMain",
    },
    fragment: {
      module: redTriangleShader,
      entryPoint: "fragmentMain",
      targets: [{ format: presentationFormat }, { format: presentationFormat } ]
    },
    primitive: { topology: "triangle-list" },
    multisample: {
      count: 4,
    }
  });

  const texture1 = device.createTexture({
    size: [canvas.width, canvas.height],
    sampleCount: 4,
    format: presentationFormat,
    usage: GPUTextureUsage.RENDER_ATTACHMENT,
  });
  const view1 = texture1.createView();
  view1.__texture = texture1;

  const texture2 = device.createTexture({
    size: [canvas.width, canvas.height],
    sampleCount: 1,
    format: presentationFormat,
    usage: GPUTextureUsage.RENDER_ATTACHMENT,
  });
  const view2 = texture2.createView();
  view2.__texture = texture2;


  const texture = device.createTexture({
    size: [canvas.width, canvas.height],
    sampleCount: 4,
    format: presentationFormat,
    usage: GPUTextureUsage.RENDER_ATTACHMENT,
  });
  const view = texture.createView();
  view.__texture = texture;

  function frame() {
    {
      // A dummy commandEncoder to test WebGPU Inspector
      const commandEncoder = device.createCommandEncoder();
      device.queue.submit([commandEncoder.finish()]);
    }

    // Create a CommandEncoder, which records all of the rendering
    // commands needed to draw something into a CommandBuffer, and
    // then you submit the CommandBuffer to do the actual rendering.
    let commandEncoder = device.createCommandEncoder();

    // For every frame we need to get the current canvas render texture.
    // The render texture will be different the next frame, as WebGPU
    // is doing a SwapChain internally. RenderPass takes a TextureView,
    // so we create a view of the canvas render texture.
    const canvasTextureView = context.getCurrentTexture().createView();

    // Start rendering to the canvas render texture.
    const passEncoder = commandEncoder.beginRenderPass({
      colorAttachments: [
        {
          view: view1,
          resolveTarget: view2,
          clearValue: { r: 0.0, g: 0.0, b: 1.0, a: 1.0 },
          loadOp: "clear",
          storeOp: "discard",
        },
        {
          view: view,
          resolveTarget: canvasTextureView,
          clearValue: { r: 0.0, g: 1.0, b: 0.0, a: 1.0 },
          loadOp: "clear",
          storeOp: "store",
        },
      ],
    });
    // Set the current render pipeline to use
    passEncoder.setPipeline(pipeline);
    // Draw 3 vertices. We"re not using a vertex buffer
    // because we have the vertex positions hard-coded in the shader.
    passEncoder.draw(3);
    // Finish drawing to the RenderPass
    passEncoder.end();

    device.queue.submit([commandEncoder.finish()]);

    commandEncoder = device.createCommandEncoder();

    const canvasTextureView2 = context.getCurrentTexture().createView();

    const passEncoder2 = commandEncoder.beginRenderPass({
      colorAttachments: [
        {
          view: view1,
          resolveTarget: view2,
          clearValue: { r: 0.0, g: 1.0, b: 1.0, a: 1.0 },
          loadOp: "clear",
          storeOp: "discard",
        },
        {
          view: view,
          resolveTarget: canvasTextureView2,
          clearValue: { r: 1.0, g: 1.0, b: 0.0, a: 1.0 },
          loadOp: "clear",
          storeOp: "store",
        },
      ],
    });
    // Set the current render pipeline to use
    passEncoder2.setPipeline(pipeline);
    // Draw 3 vertices. We"re not using a vertex buffer
    // because we have the vertex positions hard-coded in the shader.
    passEncoder2.draw(3);
    // Finish drawing to the RenderPass
    passEncoder2.end();

    // CommandEncoder.finish will put all of the commands it recorded
    // into a CommandBuffer. Queue.submit will submit that CommandBuffer
    // to the GPU for the acual rendering.
    device.queue.submit([commandEncoder.finish()]);

    // requestAnimationFrame is a Javascript mechanism for having the
    // frame function called when the next frame is ready to be rendered,
    // waiting for vsync and the browser to otherwise be ready.
    requestAnimationFrame(frame);
  }

  // Start the animation frame loop.
  requestAnimationFrame(frame);
};

main();
        </script>
    </body>
</html>
